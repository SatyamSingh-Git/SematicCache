

### ğŸ“‚ Project Root: `search-engine-assignment/`

```text
search-engine-assignment/
â”‚
â”œâ”€â”€ .gitignore                  # Standard gitignore (critical: ignores /data)
[cite_start]â”œâ”€â”€ README.md                   # Documentation, Setup, Architecture [cite: 109]
[cite_start]â”œâ”€â”€ requirements.txt            # Python dependencies [cite: 110]
â”œâ”€â”€ Dockerfile                  # Instructions to build the container
â”œâ”€â”€ docker-compose.yml          # Orchestration for API + UI + Volumes
â”œâ”€â”€ ingest.py                   # Script to run the one-time data processing pipeline
â”‚
[cite_start]â”œâ”€â”€ data/                       # DATA STORAGE (Ignored by Git) [cite: 108]
â”‚   â”œâ”€â”€ raw/                    # Place your 100-200 .txt files here
â”‚   â”œâ”€â”€ cache/                  # Stores SQLite db or JSON cache files
â”‚   â””â”€â”€ indices/                # Stores the .faiss vector index file
â”‚
[cite_start]â”œâ”€â”€ src/                        # SOURCE CODE [cite: 107]
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py               # Central config (Paths, Model Names, Constants)
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                   # CORE LOGIC MODULES
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ preprocessing.py    # Text cleaning, chunking, metadata extraction
[cite_start]â”‚   â”‚   â”œâ”€â”€ embedder.py         # Loading model, generating embeddings [cite: 66]
[cite_start]â”‚   â”‚   â”œâ”€â”€ cache_manager.py    # Hashing checks, SQL/JSON read/write [cite: 67]
[cite_start]â”‚   â”‚   â”œâ”€â”€ search_engine.py    # FAISS index management, Hybrid search logic [cite: 68]
â”‚   â”‚   â””â”€â”€ ranker.py           # (Enhancement) Cross-Encoder re-ranking logic
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                    # API LAYER
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py             # FastAPI app entry point
[cite_start]â”‚   â”‚   â”œâ”€â”€ routes.py           # The /search endpoint definition [cite: 69]
â”‚   â”‚   â””â”€â”€ schemas.py          # Pydantic models for Input/Output validation
â”‚   â”‚
â”‚   â””â”€â”€ ui/                     # USER INTERFACE
[cite_start]â”‚       â””â”€â”€ streamlit_app.py    # Streamlit frontend code [cite: 100]
â”‚
â””â”€â”€ tests/                      # UNIT TESTS (Best Practice)
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ test_preprocessing.py
    â””â”€â”€ test_search.py
```

-----

### ğŸ“ Key File Descriptions

Here is what goes into the specific files to handle the requirements and enhancements:

#### 1\. Root Level Files

  * **`ingest.py`**: This is the "SETUP" script. It runs the pipeline: Load text $\rightarrow$ Chunk $\rightarrow$ Embed $\rightarrow$ Cache $\rightarrow$ Build FAISS Index. You run this *once* before starting the API.
  * **`.gitignore`**:
    ```text
    __pycache__/
    *.pyc
    .env
    .DS_Store
    [cite_start]/data/* # CRITICAL: Assignment requires ignoring data [cite: 108]
    !/data/.gitkeep  # Keeps the folder structure in git even if empty
    ```

#### 2\. `src/config.py`

Holds your "Magic Numbers" so you don't hardcode them.

```python
import os

DATA_DIR = "data"
RAW_DATA_PATH = os.path.join(DATA_DIR, "raw")
CACHE_PATH = os.path.join(DATA_DIR, "cache", "embeddings.db")
INDEX_PATH = os.path.join(DATA_DIR, "indices", "vector.index")

# Model Settings
[cite_start]EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2" [cite: 38]
CHUNK_SIZE = 256
CHUNK_OVERLAP = 50
```

#### 3\. `src/core/preprocessing.py`

Handles the cleaning and the **Smart Chunking enhancement**.

  * **Functions:** `clean_text(text)`, `chunk_text(text)`, `get_file_hash(filepath)`.

#### 4\. `src/core/search_engine.py`

The brain of the operation.

  * **Class `SearchEngine`**:
      * `load_index()`: Loads FAISS from disk.
      * `search(query, top_k)`: Embeds query, searches FAISS.
      * **Enhancement:** Includes the `hybrid_search` logic (BM25 + Dense) inside this class.

#### 5\. `src/api/routes.py`

Handles the request/response logic.

  * **Endpoint:** `POST /search`
  * [cite\_start]**Logic:** Receives JSON, calls `SearchEngine`, formats the output with "Why this" explanations[cite: 95], and returns the JSON response.

-----

### ğŸš€ How to Start Development (Using this Structure)

1.  **Create the skeleton:**
    Run these commands in your terminal to create the structure instantly:

    ```bash
    mkdir -p search-engine-assignment/data/{raw,cache,indices}
    mkdir -p search-engine-assignment/src/{core,api,ui}
    mkdir -p search-engine-assignment/tests
    touch search-engine-assignment/src/{__init__.py,config.py}
    touch search-engine-assignment/{Dockerfile,docker-compose.yml,requirements.txt,README.md,ingest.py}
    ```

2.  **Populate `requirements.txt`:**

    ```text
    sentence-transformers
    faiss-cpu
    fastapi
    uvicorn
    streamlit
    numpy
    scikit-learn
    rank_bm25
    ```
